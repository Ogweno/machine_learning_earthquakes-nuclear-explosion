{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiral toy tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_of_layers = 4 # number of hidden layers except input and output layer\n",
    "input_layer = 7\n",
    "layer1 = 100\n",
    "layer2 = 100\n",
    "layer3 = 100\n",
    "layer4 = 100\n",
    "output_layer = 1\n",
    "output_class = 4 # Number of classes\n",
    "N = 100 # number of training examples\n",
    "learning_rate = 0.001\n",
    "training_epochs = 150\n",
    "batch_size = 10\n",
    "display_step = 1\n",
    "keep_prob = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "In this section we will generate simple spiral toy data. The function is written in such a way that it can have different number of classes based on the give input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stress_data(no_of_data):\n",
    "    data = pd.read_csv('stress_data_shuffled.csv')\n",
    "    data = data.values\n",
    "    data = data[0:no_of_data, :]\n",
    "    X = data[:no_of_data, :7] # [it looks like it is 1 indexed]\n",
    "    Y = data[:no_of_data, 9] # shear stress [it looks like it is 0 indexed]\n",
    "    return X, Y\n",
    "\n",
    "# get_stress_data(1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_stress_data(10000)\n",
    "X, Y = shuffle(X, Y, random_state=0)\n",
    "X = preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(X)\n",
    "Y = preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(Y.T)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "total_len_train = X_train.shape[0]\n",
    "total_len_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(layer1_dim, layer2_dim):\n",
    "    return tf.Variable(tf.truncated_normal([layer1_dim, layer2_dim], 0, 0.01))\n",
    "\n",
    "def bias_variable(layer_dim):\n",
    "    return tf.Variable(tf.truncated_normal([layer_dim], 0, 0.01))\n",
    "\n",
    "def variable_summaries(var, name):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean/' + name, mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev/' + name, stddev)\n",
    "    tf.summary.scalar('max/' + name, tf.reduce_max(var))\n",
    "    tf.summary.scalar('min/' + name, tf.reduce_min(var))\n",
    "    tf.summary.histogram(name, var)\n",
    "\n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "  \"\"\"Reusable code for making a simple neural net layer.\n",
    "\n",
    "  It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "  It also sets up name scoping so that the resultant graph is easy to read,\n",
    "  and adds a number of summary ops.\n",
    "  \"\"\"\n",
    "  # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "  with tf.name_scope(layer_name):\n",
    "    # This Variable will hold the state of the weights for the layer\n",
    "    with tf.name_scope('weights'):\n",
    "      weights = weight_variable(input_dim, output_dim)\n",
    "      variable_summaries(weights, layer_name + '/weights')\n",
    "    with tf.name_scope('biases'):\n",
    "      biases = bias_variable(output_dim)\n",
    "      variable_summaries(biases, layer_name + '/biases')\n",
    "    with tf.name_scope('Wx_plus_b'):\n",
    "      preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "      tf.summary.histogram(layer_name + '/pre_activations', preactivate)\n",
    "    \n",
    "    if act == None:\n",
    "        activations = preactivate\n",
    "    else:\n",
    "        activations = act(preactivate, 'activation')\n",
    "    \n",
    "    tf.summary.histogram(layer_name + '/activations', activations)\n",
    "  return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "graph1 = tf.Graph()\n",
    "with graph1.as_default():\n",
    "    # %% tf.placeholders for the input and output of the network. Placeholders are\n",
    "    # variables which we need to fill in when we are ready to compute the graph.\n",
    "    X_placeholder = tf.placeholder(tf.float32, [None, input_layer])\n",
    "    Y_placeholder = tf.placeholder(tf.float32, [None])\n",
    "#     keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # hidden layers computation  \n",
    "    hidden1 = nn_layer(X_placeholder, input_layer, layer1, 'layer1')\n",
    "    hidden1 = tf.nn.dropout(hidden1, keep_prob)\n",
    "    \n",
    "    hidden2 = nn_layer(hidden1, layer1, layer2, 'layer2')\n",
    "    hidden2 = tf.nn.dropout(hidden2, keep_prob)\n",
    "    \n",
    "    hidden3 = nn_layer(hidden2, layer2, layer3, 'layer3')\n",
    "    hidden3 = tf.nn.dropout(hidden3, keep_prob)\n",
    "    \n",
    "    hidden4 = nn_layer(hidden3, layer3, layer4, 'layer4')\n",
    "    logits  = nn_layer(hidden4, layer4, output_layer, 'output', None)\n",
    "    pred = tf.transpose(logits)\n",
    "    \n",
    "    #  cost : squred mean   \n",
    "    with tf.name_scope('cost'):         \n",
    "        cost = tf.reduce_mean(tf.square(pred - Y_placeholder))\n",
    "        tf.summary.scalar('cost', cost)\n",
    "        \n",
    "    #  Accuracy : squred mean   \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.subtract(tf.cast(1, 'float'), tf.reduce_mean(tf.subtract(pred, Y_placeholder)))\n",
    "        accuracy = tf.cast(correct_prediction, \"float\")\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "                       \n",
    "    #optimizer\n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('traning epoch : ', 0, 'train loss : ', 0.005384245, 'test loss :', 0.0045885616)\n",
      "('traning epoch : ', 1, 'train loss : ', 0.0050126957, 'test loss :', 0.0046040635)\n",
      "('traning epoch : ', 2, 'train loss : ', 0.0042492221, 'test loss :', 0.0044569196)\n",
      "('traning epoch : ', 3, 'train loss : ', 0.0042396327, 'test loss :', 0.0046626953)\n",
      "('traning epoch : ', 4, 'train loss : ', 0.0043349485, 'test loss :', 0.0047025373)\n",
      "('traning epoch : ', 5, 'train loss : ', 0.0044457437, 'test loss :', 0.0046986202)\n",
      "('traning epoch : ', 6, 'train loss : ', 0.0045018164, 'test loss :', 0.0045455857)\n",
      "('traning epoch : ', 7, 'train loss : ', 0.0044925041, 'test loss :', 0.0044398201)\n",
      "('traning epoch : ', 8, 'train loss : ', 0.0044024251, 'test loss :', 0.0043852022)\n",
      "('traning epoch : ', 9, 'train loss : ', 0.0043473402, 'test loss :', 0.0044378005)\n",
      "('traning epoch : ', 10, 'train loss : ', 0.0042155152, 'test loss :', 0.0045892051)\n",
      "('traning epoch : ', 11, 'train loss : ', 0.0040659062, 'test loss :', 0.0045834212)\n",
      "('traning epoch : ', 12, 'train loss : ', 0.0039508045, 'test loss :', 0.0045739287)\n",
      "('traning epoch : ', 13, 'train loss : ', 0.0039163483, 'test loss :', 0.0047962279)\n",
      "('traning epoch : ', 14, 'train loss : ', 0.0038098842, 'test loss :', 0.0049689217)\n",
      "('traning epoch : ', 15, 'train loss : ', 0.0037440814, 'test loss :', 0.0052940184)\n",
      "('traning epoch : ', 16, 'train loss : ', 0.0037581872, 'test loss :', 0.005688672)\n",
      "('traning epoch : ', 17, 'train loss : ', 0.0037200688, 'test loss :', 0.0059418841)\n",
      "('traning epoch : ', 18, 'train loss : ', 0.0037596144, 'test loss :', 0.0060724863)\n",
      "('traning epoch : ', 19, 'train loss : ', 0.0037532591, 'test loss :', 0.0060419803)\n",
      "('traning epoch : ', 20, 'train loss : ', 0.0038099282, 'test loss :', 0.006122963)\n",
      "('traning epoch : ', 21, 'train loss : ', 0.0038351256, 'test loss :', 0.0059104245)\n",
      "('traning epoch : ', 22, 'train loss : ', 0.0037307281, 'test loss :', 0.006024099)\n",
      "('traning epoch : ', 23, 'train loss : ', 0.0038533818, 'test loss :', 0.0056523439)\n",
      "('traning epoch : ', 24, 'train loss : ', 0.0037329297, 'test loss :', 0.0056403927)\n",
      "('traning epoch : ', 25, 'train loss : ', 0.0037526104, 'test loss :', 0.0057939002)\n",
      "('traning epoch : ', 26, 'train loss : ', 0.0037633781, 'test loss :', 0.0057222312)\n",
      "('traning epoch : ', 27, 'train loss : ', 0.003753752, 'test loss :', 0.0057626595)\n",
      "('traning epoch : ', 28, 'train loss : ', 0.0037104927, 'test loss :', 0.0056831287)\n",
      "('traning epoch : ', 29, 'train loss : ', 0.0036928304, 'test loss :', 0.0053115073)\n",
      "('traning epoch : ', 30, 'train loss : ', 0.0037247483, 'test loss :', 0.0054420903)\n",
      "('traning epoch : ', 31, 'train loss : ', 0.0037506961, 'test loss :', 0.0052578822)\n",
      "('traning epoch : ', 32, 'train loss : ', 0.0038169057, 'test loss :', 0.0052843504)\n",
      "('traning epoch : ', 33, 'train loss : ', 0.0038074113, 'test loss :', 0.0050206259)\n",
      "('traning epoch : ', 34, 'train loss : ', 0.0037873972, 'test loss :', 0.0050192117)\n",
      "('traning epoch : ', 35, 'train loss : ', 0.0039122682, 'test loss :', 0.0050401948)\n",
      "('traning epoch : ', 36, 'train loss : ', 0.0037900996, 'test loss :', 0.0049659889)\n",
      "('traning epoch : ', 37, 'train loss : ', 0.0038802966, 'test loss :', 0.0048238579)\n",
      "('traning epoch : ', 38, 'train loss : ', 0.003915797, 'test loss :', 0.0048979465)\n",
      "('traning epoch : ', 39, 'train loss : ', 0.0038670257, 'test loss :', 0.0048395041)\n",
      "('traning epoch : ', 40, 'train loss : ', 0.0039136875, 'test loss :', 0.0049579646)\n",
      "('traning epoch : ', 41, 'train loss : ', 0.0038198859, 'test loss :', 0.0048698289)\n",
      "('traning epoch : ', 42, 'train loss : ', 0.0037858922, 'test loss :', 0.0048938869)\n",
      "('traning epoch : ', 43, 'train loss : ', 0.0037682999, 'test loss :', 0.0049418746)\n",
      "('traning epoch : ', 44, 'train loss : ', 0.0038301386, 'test loss :', 0.0050152843)\n",
      "('traning epoch : ', 45, 'train loss : ', 0.0037400902, 'test loss :', 0.0047932505)\n",
      "('traning epoch : ', 46, 'train loss : ', 0.0037786639, 'test loss :', 0.0049835932)\n",
      "('traning epoch : ', 47, 'train loss : ', 0.003653887, 'test loss :', 0.0046869661)\n",
      "('traning epoch : ', 48, 'train loss : ', 0.003488116, 'test loss :', 0.0048776707)\n",
      "('traning epoch : ', 49, 'train loss : ', 0.003716988, 'test loss :', 0.0049498389)\n",
      "('traning epoch : ', 50, 'train loss : ', 0.0036686566, 'test loss :', 0.005171082)\n",
      "('traning epoch : ', 51, 'train loss : ', 0.0035982572, 'test loss :', 0.00494006)\n",
      "('traning epoch : ', 52, 'train loss : ', 0.0037960368, 'test loss :', 0.0049426481)\n",
      "('traning epoch : ', 53, 'train loss : ', 0.0037838214, 'test loss :', 0.0048636971)\n",
      "('traning epoch : ', 54, 'train loss : ', 0.0037885525, 'test loss :', 0.0050274893)\n",
      "('traning epoch : ', 55, 'train loss : ', 0.0036797351, 'test loss :', 0.004887864)\n",
      "('traning epoch : ', 56, 'train loss : ', 0.0037389968, 'test loss :', 0.0049714376)\n",
      "('traning epoch : ', 57, 'train loss : ', 0.0036432869, 'test loss :', 0.0050891065)\n",
      "('traning epoch : ', 58, 'train loss : ', 0.0037665754, 'test loss :', 0.0049604084)\n",
      "('traning epoch : ', 59, 'train loss : ', 0.0037203841, 'test loss :', 0.0048782737)\n",
      "('traning epoch : ', 60, 'train loss : ', 0.00364578, 'test loss :', 0.0046407231)\n",
      "('traning epoch : ', 61, 'train loss : ', 0.0035396335, 'test loss :', 0.0050590993)\n",
      "('traning epoch : ', 62, 'train loss : ', 0.0036530006, 'test loss :', 0.0048080562)\n",
      "('traning epoch : ', 63, 'train loss : ', 0.0036881422, 'test loss :', 0.0049757306)\n",
      "('traning epoch : ', 64, 'train loss : ', 0.0035997094, 'test loss :', 0.0048268242)\n",
      "('traning epoch : ', 65, 'train loss : ', 0.0035217572, 'test loss :', 0.0046801427)\n",
      "('traning epoch : ', 66, 'train loss : ', 0.0036447376, 'test loss :', 0.0047991807)\n",
      "('traning epoch : ', 67, 'train loss : ', 0.0038239956, 'test loss :', 0.0049663195)\n",
      "('traning epoch : ', 68, 'train loss : ', 0.0037147671, 'test loss :', 0.004867238)\n",
      "('traning epoch : ', 69, 'train loss : ', 0.003701064, 'test loss :', 0.0048095831)\n",
      "('traning epoch : ', 70, 'train loss : ', 0.0039334223, 'test loss :', 0.0047298814)\n",
      "('traning epoch : ', 71, 'train loss : ', 0.0037011853, 'test loss :', 0.0049675093)\n",
      "('traning epoch : ', 72, 'train loss : ', 0.0036194369, 'test loss :', 0.0047856318)\n",
      "('traning epoch : ', 73, 'train loss : ', 0.0037632559, 'test loss :', 0.004822609)\n",
      "('traning epoch : ', 74, 'train loss : ', 0.0037738741, 'test loss :', 0.0047681769)\n",
      "('traning epoch : ', 75, 'train loss : ', 0.0037439517, 'test loss :', 0.004863861)\n",
      "('traning epoch : ', 76, 'train loss : ', 0.0038780258, 'test loss :', 0.0048978454)\n",
      "('traning epoch : ', 77, 'train loss : ', 0.0036155719, 'test loss :', 0.0047352654)\n",
      "('traning epoch : ', 78, 'train loss : ', 0.0036259056, 'test loss :', 0.0047153546)\n",
      "('traning epoch : ', 79, 'train loss : ', 0.0035243358, 'test loss :', 0.0045878291)\n",
      "('traning epoch : ', 80, 'train loss : ', 0.0037982841, 'test loss :', 0.0047295801)\n",
      "('traning epoch : ', 81, 'train loss : ', 0.0037491515, 'test loss :', 0.0048527243)\n",
      "('traning epoch : ', 82, 'train loss : ', 0.0037821524, 'test loss :', 0.0047518839)\n",
      "('traning epoch : ', 83, 'train loss : ', 0.0036932467, 'test loss :', 0.0048862915)\n",
      "('traning epoch : ', 84, 'train loss : ', 0.0037086853, 'test loss :', 0.0049489811)\n",
      "('traning epoch : ', 85, 'train loss : ', 0.0037516966, 'test loss :', 0.0048295343)\n",
      "('traning epoch : ', 86, 'train loss : ', 0.0038954732, 'test loss :', 0.004769369)\n",
      "('traning epoch : ', 87, 'train loss : ', 0.003827082, 'test loss :', 0.0045691668)\n",
      "('traning epoch : ', 88, 'train loss : ', 0.0035576106, 'test loss :', 0.0047672442)\n",
      "('traning epoch : ', 89, 'train loss : ', 0.0037616522, 'test loss :', 0.0045232647)\n",
      "('traning epoch : ', 90, 'train loss : ', 0.0037957546, 'test loss :', 0.0047058305)\n",
      "('traning epoch : ', 91, 'train loss : ', 0.0038532347, 'test loss :', 0.0045746723)\n",
      "('traning epoch : ', 92, 'train loss : ', 0.0038109079, 'test loss :', 0.0048241834)\n",
      "('traning epoch : ', 93, 'train loss : ', 0.0037073083, 'test loss :', 0.0046718824)\n",
      "('traning epoch : ', 94, 'train loss : ', 0.0037171976, 'test loss :', 0.0049739694)\n",
      "('traning epoch : ', 95, 'train loss : ', 0.003838941, 'test loss :', 0.0050284937)\n",
      "('traning epoch : ', 96, 'train loss : ', 0.0039816005, 'test loss :', 0.0050916108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('traning epoch : ', 97, 'train loss : ', 0.0036712307, 'test loss :', 0.0049706423)\n",
      "('traning epoch : ', 98, 'train loss : ', 0.0037552014, 'test loss :', 0.0050330386)\n",
      "('traning epoch : ', 99, 'train loss : ', 0.0040082787, 'test loss :', 0.0053487546)\n",
      "('traning epoch : ', 100, 'train loss : ', 0.0039977464, 'test loss :', 0.0050052088)\n",
      "('traning epoch : ', 101, 'train loss : ', 0.0040713912, 'test loss :', 0.0051832986)\n",
      "('traning epoch : ', 102, 'train loss : ', 0.004032976, 'test loss :', 0.0050850683)\n",
      "('traning epoch : ', 103, 'train loss : ', 0.0040120063, 'test loss :', 0.0050800084)\n",
      "('traning epoch : ', 104, 'train loss : ', 0.0039643054, 'test loss :', 0.004893356)\n",
      "('traning epoch : ', 105, 'train loss : ', 0.0039492869, 'test loss :', 0.0049383836)\n",
      "('traning epoch : ', 106, 'train loss : ', 0.0038998569, 'test loss :', 0.0050191358)\n",
      "('traning epoch : ', 107, 'train loss : ', 0.004044889, 'test loss :', 0.0047242679)\n",
      "('traning epoch : ', 108, 'train loss : ', 0.0039800964, 'test loss :', 0.0048464001)\n",
      "('traning epoch : ', 109, 'train loss : ', 0.0038655133, 'test loss :', 0.0050378982)\n",
      "('traning epoch : ', 110, 'train loss : ', 0.0040295706, 'test loss :', 0.004996798)\n",
      "('traning epoch : ', 111, 'train loss : ', 0.00377507, 'test loss :', 0.0048445938)\n",
      "('traning epoch : ', 112, 'train loss : ', 0.0038613894, 'test loss :', 0.0048083849)\n",
      "('traning epoch : ', 113, 'train loss : ', 0.0038029761, 'test loss :', 0.0048455573)\n",
      "('traning epoch : ', 114, 'train loss : ', 0.0038352576, 'test loss :', 0.0047175488)\n",
      "('traning epoch : ', 115, 'train loss : ', 0.0037565548, 'test loss :', 0.0049286066)\n",
      "('traning epoch : ', 116, 'train loss : ', 0.0037554398, 'test loss :', 0.0048099314)\n",
      "('traning epoch : ', 117, 'train loss : ', 0.0037154597, 'test loss :', 0.004927828)\n",
      "('traning epoch : ', 118, 'train loss : ', 0.0036946354, 'test loss :', 0.0050240881)\n",
      "('traning epoch : ', 119, 'train loss : ', 0.0037606333, 'test loss :', 0.0050038723)\n",
      "('traning epoch : ', 120, 'train loss : ', 0.0034880298, 'test loss :', 0.0047297841)\n",
      "('traning epoch : ', 121, 'train loss : ', 0.0035378127, 'test loss :', 0.0046900641)\n",
      "('traning epoch : ', 122, 'train loss : ', 0.003472799, 'test loss :', 0.0047631422)\n",
      "('traning epoch : ', 123, 'train loss : ', 0.0034270461, 'test loss :', 0.0046480326)\n",
      "('traning epoch : ', 124, 'train loss : ', 0.0035232487, 'test loss :', 0.0048092632)\n",
      "('traning epoch : ', 125, 'train loss : ', 0.0034033742, 'test loss :', 0.0047366321)\n",
      "('traning epoch : ', 126, 'train loss : ', 0.0032909985, 'test loss :', 0.0046230117)\n",
      "('traning epoch : ', 127, 'train loss : ', 0.0034979456, 'test loss :', 0.0047143851)\n",
      "('traning epoch : ', 128, 'train loss : ', 0.0034478293, 'test loss :', 0.0049636541)\n",
      "('traning epoch : ', 129, 'train loss : ', 0.003419938, 'test loss :', 0.0048415153)\n",
      "('traning epoch : ', 130, 'train loss : ', 0.0035572976, 'test loss :', 0.0047446834)\n",
      "('traning epoch : ', 131, 'train loss : ', 0.0036321543, 'test loss :', 0.0048838248)\n",
      "('traning epoch : ', 132, 'train loss : ', 0.0036895878, 'test loss :', 0.0048476816)\n",
      "('traning epoch : ', 133, 'train loss : ', 0.0037127347, 'test loss :', 0.0048063761)\n",
      "('traning epoch : ', 134, 'train loss : ', 0.0031759285, 'test loss :', 0.0048829159)\n",
      "('traning epoch : ', 135, 'train loss : ', 0.0035088304, 'test loss :', 0.00470697)\n",
      "('traning epoch : ', 136, 'train loss : ', 0.0032987334, 'test loss :', 0.0047402689)\n",
      "('traning epoch : ', 137, 'train loss : ', 0.0034799178, 'test loss :', 0.0045634964)\n",
      "('traning epoch : ', 138, 'train loss : ', 0.00329408, 'test loss :', 0.004672321)\n",
      "('traning epoch : ', 139, 'train loss : ', 0.003259676, 'test loss :', 0.0046957317)\n",
      "('traning epoch : ', 140, 'train loss : ', 0.0034090385, 'test loss :', 0.0049324944)\n",
      "('traning epoch : ', 141, 'train loss : ', 0.0035016469, 'test loss :', 0.0046750698)\n",
      "('traning epoch : ', 142, 'train loss : ', 0.0036496795, 'test loss :', 0.0047191037)\n",
      "('traning epoch : ', 143, 'train loss : ', 0.0037038072, 'test loss :', 0.0046178224)\n",
      "('traning epoch : ', 144, 'train loss : ', 0.0036354109, 'test loss :', 0.004690018)\n",
      "('traning epoch : ', 145, 'train loss : ', 0.0034573455, 'test loss :', 0.0045037731)\n",
      "('traning epoch : ', 146, 'train loss : ', 0.0035642416, 'test loss :', 0.0044047111)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of ./checkpoints/spiral doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ca4c7e335f91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# saving the entire graph and its variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchkpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1380\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m       raise ValueError(\n\u001b[0;32m-> 1382\u001b[0;31m           \"Parent directory of {} doesn't exist, can't save.\".format(save_path))\n\u001b[0m\u001b[1;32m   1383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parent directory of ./checkpoints/spiral doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "def feed_dict(train, i):\n",
    "  \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "  if (train):\n",
    "    xs = X_train[i*batch_size:(i+1)*batch_size]\n",
    "    ys = Y_train[i*batch_size:(i+1)*batch_size]\n",
    "  else:\n",
    "    xs = X_test[i*batch_size:(i+1)*batch_size]\n",
    "    ys = Y_test[i*batch_size:(i+1)*batch_size]\n",
    "  return {X_placeholder: xs, Y_placeholder: ys}\n",
    "\n",
    "\n",
    "# %% We create a session to use the graph\n",
    "with tf.Session(graph=graph1) as sess:\n",
    "    \n",
    "    # save all the varibales and weights of this graph\n",
    "    saver = tf.train.Saver()\n",
    "    chkpt_path = \"./checkpoints/spiral\"\n",
    "    \n",
    "    # check if the there is an existing chkpt files\n",
    "#     ckpt = tf.train.get_checkpoint_state(\"./checkpoints\")\n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('./checkpoints'))\n",
    "#     print(\"checkpoint: \", ckpt)\n",
    "        \n",
    "    # -----prepare tensor board visualization variables/weights --------\n",
    "    # Merge all the summaries and write them out to output dir\n",
    "    now = datetime.now()\n",
    "    train_sub = 'train_' + now.strftime(\"%Y%m%d-%H%M\")+'_drop_' + str(keep_prob)+'_layer_'+ str(no_of_layers) +'_data_'+ str(N)+ '_neuron_' + str(layer1) \n",
    "    test_sub = 'test_' + now.strftime(\"%Y%m%d-%H%M\")+'_drop_' + str(keep_prob)+'_layer_'+ str(no_of_layers) +'_data_'+ str(N)+ '_neuron_' + str(layer1)\n",
    "    \n",
    "    train_path = './train/' + str(train_sub)\n",
    "    test_path = './test/' + str(test_sub)\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(train_path, sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(test_path, sess.graph)\n",
    "    # --------------- ######### --------------\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    trainng_loss = []\n",
    "    epoch1 = []\n",
    "    testing_loss = []\n",
    "    epoch2 = []\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        ## Training      \n",
    "        total_batch = int(total_len_train/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            summary_train, _, a_train, loss_train, p_train = sess.run([merged, optimizer, accuracy, cost, pred], feed_dict = feed_dict(True, i))\n",
    "        trainng_loss.append(loss_train)\n",
    "        epoch1.append(epoch)\n",
    "        train_writer.add_summary(summary_train, epoch)\n",
    "        \n",
    "        ## Testing\n",
    "        if (epoch%1 == 0):\n",
    "            total_batch = int(total_len_test/batch_size)\n",
    "            p_test = []\n",
    "            for i in range(total_batch):\n",
    "                summary_test, _, a_test, loss_test, p_test = sess.run([merged, optimizer, accuracy, cost, pred], feed_dict = feed_dict(False, i))\n",
    "                p_test\n",
    "            testing_loss.append(loss_test)\n",
    "            epoch2.append(epoch)\n",
    "            test_writer.add_summary(summary_test, epoch)\n",
    "        \n",
    "        # printing  loss  \n",
    "        if (epoch%1 == 0):\n",
    "            print(\"traning epoch : \", epoch, \"train loss : \",  loss_train, \"test loss :\", loss_test)\n",
    "        \n",
    "        # saving the entire graph and its variables\n",
    "        if (epoch > (training_epochs-5)):\n",
    "            saver.save(sess, chkpt_path, global_step=epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "## Some conclusions\n",
    "####  Three class problems :\n",
    "Overfitting can be solved by introducing dropout rules, however it does not work always as it is. For example if we have only two classes and four hidden layers and each layer has 200 neurons, 0.5 keep_prob trainng drop out does not help. If we decrease number of neurons to 100 overfitting reduced gives better result.\n",
    "\n",
    "#### Four class problems :\n",
    "1. Since we have more classes, we have to have to little more neurons in each layers to learn more while keeping the same number of hidden layer. \n",
    "2. 0.30 keep_prob makes underfitting\n",
    "3. 0.35 keep_prob makes almost perfect fitting\n",
    "4. 0.40 keep_prob makes almost the same fitting\n",
    "5. 0.40 with 1000 trainning epoch makes the tesing loss curve underneath the training curve.\n",
    "6. To overcome this problem we can try increasing keep_prob or number of layers or may be both.\n",
    "7. We increased keep_prob to 0.6, not sure if its improved the result\n",
    "lets try with decreasing no. of neurons to 150 in each layer keeping all the parameters same.\n",
    "8. No. of neurons 250, validation curve is less\n",
    "Still no good results. May be we have to increase number of layers.\n",
    "\n",
    "9. 0.6 keep_prob and 250 neurons in each layer makes the both curve nice and adjustable, althouth test curve is little low than the traiing curve\n",
    "\n",
    "10. increase number of datasets N= 4000 from 500. keep_prob=1.0\n",
    "\n",
    "### possible reasons :\n",
    "This is almost always due to the small size of test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
